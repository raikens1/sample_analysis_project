---
title: "Basic Models"
author: "Rachael Caelie (Rocky) Aikens"
date: "6/10/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.align = "center")
library(confirmationBias)
library(tidyverse)
library(ggpubr)
library(knitr)
library(binom)
theme_set(theme_light())

n <- 50000
```

# A Simple Model with Diagnosis by Severity

Suppose we want to do a study of a disease. Perhaps our target population is "all people," or perhaps it is some subset of people (e.g. only Men, only Asians, etc.). Each person, $i$, has an underlying (unobserved) disease state $Z_i$ and a disease severity $S_i$.  A simple model is:

\begin{align*}
    Z_i &\sim_{iid} \text{Bernoulli}(0.15)\\
    S_i|Z_i = 1 &\sim_{iid} \text{Uniform}(0, 1)\\
\end{align*}

Where $S_i = 0$ whenever $Z_i = 0$. 

Suppose additionally that a person's likelihood of diagnosis is a function of the severity of their illness, namely

$$D_i \sim_{iid} \text{Bernoulli}\left(\phi(S_i\right))$$

$\phi(S_i)$ could be many possible functions, here I use a sigmoid function:

$$\phi(S_i) = \frac{1}{1 + exp(-(\beta_0 + \beta_1S_i))} - c$$
Here, $c = \frac{1}{1 + exp(-\beta_0)}$ is a corrective constant to ensure that $\phi(0) = 0$. Let $\beta_1 = 20$ and $\beta_0 = -10$.  This gives the following probability of diagnosis as a function of severity.

```{r fig.height=3, fig.width=3}
S <- seq(0, 1, length.out = 100)

ggplot(data = data.frame(S), aes(x = S, y = diagnosis_fn(S, 0, theta = c(-10, 20, 0)))) +
  geom_line() + xlim(c(0,1)) + ylim(c(0,1)) + coord_fixed()+
  ylab("P(Diagnosis | S)")
```

A few example rows sampled from the above distribution are printed below:

```{r}
set.seed(123)

n <- 50000
df <- generate_cross_sectional(n = n) %>%
  diagnose_cross_sectional(useX = F)

df %>% select(-x) %>% head()
```

\pagebreak

# A Naive Study

Let's suppose that a naive researcher would like to study this disease.  In particular, they want to ask

1. What is the prevalence of the disease?

2. What is the distribution of severities of this disease?

This researcher collects `r n` records from the EHR, $\left\{S_i, D_i\right\}_{i = 1}^n$.  We'll assume for the moment that the individuals represented in the EHR are representative of the target population (this is almost certainly false - it's inevitable that there are types of people that systematically tend not to appear in the EHR at all). We will suppose that not all information about all of these individuals is recorded (i.e. the underlying disease states for all individuals are not known). 

Because they observe the diagnoses but not the underlying disease states, the naive researcher considers only the diagnosed people to have the disease.  

### Prevalence

Out of `r n` people, `r sum(df$diagnose)` are diagnosed, so they estimate the prevalence to be `r sum(df$diagnose)/n`.  Because they collected so many samples, they are quite sure of this number. The table below shows the confidence intervals that this researcher might calculate based on their approach.

```{r}
binom.confint(sum(df$diagnose), n, method = "asymptotic") %>% kable(digits = 3)
```

### Severity Distrubution

Now they want to understand the severity of the disease.  Let's suppose for the moment that we have a straightforward measure of disease severity between 0 and 1.  The researcher calls any severity less than 1/3 "low" severity, 1/3 - 2/3 "medium" severity and 2/3-1 "high" severity.  From these counts, they conclude that this disease more frequently occurs at high severities, and that mild forms of the disease are rare.

```{r}
df %>%
  filter(diagnosed == T) %>%
  mutate(severity_grp = ifelse(severity < 1/3, "low", ifelse(severity < 2/3, "medium", "high"))) %>%
  group_by(severity_grp) %>%
  summarize(count = n(), prevalence = n()/sum(df$diagnose)) %>%
  arrange(count) %>%
  kable(digits = 2)
```

Finally, they want an estimate of median severity and quantiles.  They find that the median severity is quite high, based on the severity quantiles in the diagnosed people, below

```{r}
df %>% filter(diagnosed == T) %>% pull(severity) %>% quantile() %>% kable()
```

The researcher concludes that this is a disease that appears in about `r round(sum(df$diagnose)/n * 100, digits = 2)`% of the population and that when it appears it tends to be severe.

\pagebreak

## What's wrong with this approach

The issue here is that the researcher acts as though everyone who has the disease is diagnosed.  In their calculations of prevalence they are estimating $P(D_i = 1)$, but they imagine they are estimating $P(Z_i = 1)$.  In their calculations of severity they imagine they are understanding $P(S_i)$, when in fact they are estimating $P(S_i | D_i = 1).$

Under this model, we have a disease prevalence of 15% and a diagnosis rate of about 50%.  Already we can see how misdiagnosis can distort our perception of the disease - If we consider only the people being diagnosed for the disease, the disease prevalence appears to be about half what it actually is.

```{r}
df %>% group_by(disease) %>% summarize(Total = n(), Diagnosed = sum(diagnosed)) %>% kable()
```

This diagnosis process distorts the distribution of severities we observe.  

```{r}
diseased <- df %>% filter(disease == TRUE) %>%
  mutate(group = "Individuals with Disease", alpha = 0.7)

diagnosed <- df %>% filter(diagnosed == TRUE) %>%
  mutate(group = "Individuals Diagnosed", alpha = 1)

ggplot(rbind(diseased, diagnosed), aes(x = severity, group = group)) +
  geom_histogram(aes(alpha = alpha), position = "identity") +
  scale_alpha(range = c(0.4, 0.8))
```


\pagebreak

# What Happens When we Sample

Suppose up until now that every diagnosis has been made with a diagnostic test, which we'll assume for now has perfect sensitivity and specificity.  Now suppose that for 5% of the population we decide to test them anyway, regardless of the severity of their symptoms, we retrieve the uniform distribution in the randomly tested sample.

```{r}
df_sample <- df %>%
  mutate(p_sample = runif(n)) %>%
  filter(p_sample < 0.05) %>%
  mutate(new_diagnosed = ifelse(disease == 1, 1, 0))

diseased <- df_sample %>% filter(disease == TRUE) %>%
  mutate(group = "Individuals with Disease", alpha = 0.7)

diagnosed <- df_sample %>% filter(new_diagnosed == TRUE) %>%
  mutate(group = "Individuals Diagnosed", alpha = 1)

ggplot(rbind(diseased, diagnosed), aes(x = severity, group = group)) +
  geom_histogram(aes(alpha = alpha), position = "identity") +
  scale_alpha(range = c(0.4, 0.8))
```

From this random sample we can obtain a new estimate of disease prevalence (with wider confidence intervals now because of the smaller sample)

```{r}
sample.CI <- binom.confint(sum(df_sample$new_diagnose), dim(df_sample)[1], method = "asymptotic") 

sample.CI %>% kable(digits = 3)

prev <- sample.CI$mean
prev.lowerCI <- sample.CI$lower
prev.upperCI <- sample.CI$upper
```

```{r}
est_total <- n * sample.CI %>% select(mean, lower, upper) 

colnames(est_total) = c("Estimate", "Lower CI", "Upper CI")

actual_total <- c(sum(df$disease), NA, NA)

est_missed <- est_total - sum(df$diagnosed)

actual_missed <- c(sum(df$diagnosed), NA, NA)


tabledata <- rbind(est_total, est_missed) %>%
  mutate(Actual = c(sum(df$disease), sum(df$disease) - sum(df$diagnosed))) %>%
  select(Actual, Estimate, `Lower CI`, `Upper CI`)

rownames(tabledata) <- c("Total Cases", "Missed Cases")

kable(tabledata, digits = 0)
```



\pagebreak

# Some Maths

Here's some maths that might be useful...? $i$ subscripts are dropped for simplicity

$$P(D = 1|S) = \frac{P(S|D = 1)P(D)}{P(S)}$$

It seems like all the quantities on the right hand side are knowable quantities, and it would be nice to understand the left hand side so we could understand how this distorts our measurement process.  Moreover

$$P(S) = P(S|Z = 1)P(Z = 1) + P(S|Z = 0)P(Z = 0)$$
I'm less clear on how this would be useful to us, but the left hand side could be a knowable quantity and the right hand side is all things it would be nice to know.  If we can reasonably assume that $P(S|Z = 0) = 0$ (i.e. the symptoms/severity in question cannot be explained by any other disease), then the problem simplifies substantially.  Then we could estimate the number of symptomatic cases by identifying all people with $S \neq 0$, and the problem reduces to identifying the cases where $Z = 1$ but $S = 0$.


Also

$$P(Z) = \frac{P(DZ)}{P(D|Z)} = \frac{P(Z|D)P(D)}{P(D|Z)}$$
Which seems useful?  Because we might be able to get a handle on $P(Z|D)$ from our understanding of whatever diagnostic criteria we use, and $P(D)$ is estimable from the data.


# Things that could be changed about this set-up

Even at this simple stage there are tweaks that could be made.  Here are some:

1. We could vary the distribution of severities $S_i | Z_i = 1$.  For example these could be normal (most people medium ill), right tailed (most people mildly ill), or left-tailed (most people very ill).  There is no reason the severity must be between zero and 1.

2. We could vary the probability of diagnosis based on severity, $\phi(S_i)$

3. Rather than considering severity, we could consider some number of discrete symptoms, $W_{i1}, ....W_{ik}$, and have $\phi(W_{i1}, ....W_{ik})$.
